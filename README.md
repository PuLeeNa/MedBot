# ğŸ¥ MedBot

## ğŸŒ Live Demo

**Try it now:** [https://medic-bot.netlify.app](https://medic-bot.netlify.app)

An intelligent medical chatbot powered by **Groq API (Llama 3.3 70B)**, **LangChain**, and **Pinecone** vector database. This RAG (Retrieval-Augmented Generation) application provides accurate medical information by retrieving relevant context from medical documents and generating fast, high-quality responses using Groq's lightning-fast inference API.

![Python](https://img.shields.io/badge/Python-3.10-blue)
![Flask](https://img.shields.io/badge/Flask-3.1.0-green)
![LangChain](https://img.shields.io/badge/LangChain-1.1.0-orange)

## âœ¨ Features

- ğŸ¤– **RAG-based Architecture**: Retrieves relevant medical information from documents before generating responses
- âš¡ **Lightning-Fast API**: Powered by Groq's ultra-fast inference (up to 10x faster than local models)
- ğŸ¯ **Vector Search**: Uses Pinecone for efficient semantic search across medical documents
- ğŸ’¬ **Interactive UI**: Clean, responsive chat interface with typing indicators
- ğŸ†“ **Free Tier Available**: Groq offers generous free API access (14,400 requests/day)
- ğŸš€ **High-Quality Responses**: Uses Llama 3.3 70B model for superior accuracy
- ğŸ“š **Document Support**: Processes PDF medical documents with PyMuPDF
- ğŸ’¡ **Smart Caching**: Common responses feature for frequently asked questions

## ğŸ—ï¸ Architecture

**Split Deployment:**

- **Frontend**: Static files on Netlify
- **Backend**: Flask API on Render (handles AI processing)

This is a **RAG (Retrieval-Augmented Generation)** chatbot:

1. **Retrieval**: Searches Pinecone vector database for relevant medical document chunks
2. **Augmentation**: Injects retrieved context into the prompt template
3. **Generation**: Groq API (Llama 3.3 70B) generates fast, accurate responses based on the context

```
User â†’ Netlify Frontend â†’ Render Backend API â†’ Pinecone â†’ Groq LLM â†’ Response
```

## ğŸ› ï¸ Tech Stack

### Core Technologies

- **LLM**: Groq API - Llama 3.3 70B Versatile
- **Framework**: LangChain 1.1.0
- **Vector DB**: Pinecone
- **Embeddings**: HuggingFace sentence-transformers (all-MiniLM-L6-v2)
- **Backend**: Flask 3.1.0
- **Model Inference**: langchain-groq 1.1.0

### Key Libraries

- `langchain-groq` - Groq API integration for fast LLM inference
- `langchain-community` - Document loaders and LLM integrations
- `langchain-pinecone` - Pinecone vector store integration
- `langchain-huggingface` - HuggingFace embeddings
- `sentence-transformers` - Text embedding models
- `pymupdf` - PDF document processing
- `python-dotenv` - Environment variable management

## ğŸ“¦ Prerequisites

- Python 3.10 or higher
- Conda (Anaconda/Miniconda) - Optional
- Pinecone API key ([Get it here](https://www.pinecone.io/))
- Groq API key ([Get free key here](https://console.groq.com/))

## ğŸš€ Quick Deploy

### Backend (Render)

1. Push to GitHub main branch
2. Auto-deploys via `deploy.yml`
3. Render will auto-detect `render.yaml` configuration

### Frontend (Netlify)

1. Push to GitHub main branch
2. Auto-deploys via Netlify configuration
3. Netlify will auto-detect `netlify.toml` configuration

## ğŸ’» Local Development

### Backend Setup

```bash
cd backend
conda create -n mchatbot python=3.10 -y
conda activate mchatbot
pip install -r requirements.txt
python app.py
```

### Frontend Setup

```bash
cd frontend
python -m http.server 8000
# Visit: http://localhost:8000
```

## âš™ï¸ Configuration

### Environment Variables

**Backend (.env)**:

```bash
PINECONE_API_KEY=your_pinecone_api_key_here
GROQ_API_KEY=your_groq_api_key_here
```

**Frontend (app.js)**:

```javascript
const API_URL = "https://your-backend-url.onrender.com";
```

### Get Free API Keys

**Groq API**:

1. Visit [https://console.groq.com/](https://console.groq.com/)
2. Sign up for free account
3. Create API key and copy it

**Pinecone**:

1. Visit [https://www.pinecone.io/](https://www.pinecone.io/)
2. Sign up for free account
3. Create index named "medical-chatbotn"

## ğŸ“ Project Structure

```
MedBot/
â”‚
â”œâ”€â”€ backend/                    # Backend API
â”‚   â”œâ”€â”€ app.py                  # Flask API server
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile              # Container config
â”‚   â”œâ”€â”€ data/                   # Medical PDF documents
â”‚   â””â”€â”€ src/                    # Source modules
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ common_responses.py # Cached responses
â”‚       â”œâ”€â”€ helper.py           # Utility functions
â”‚       â””â”€â”€ prompt.py           # Prompt templates
â”‚
â”œâ”€â”€ frontend/                   # Frontend UI
â”‚   â”œâ”€â”€ index.html              # Chat interface
â”‚   â”œâ”€â”€ app.js                  # API client logic
â”‚   â”œâ”€â”€ style.css               # Styling
â”‚   â”œâ”€â”€ logo.jpg                # Logo/favicon
â”‚   â”œâ”€â”€ Dockerfile              # Container config
â”‚   â””â”€â”€ netlify.toml            # Netlify config
â”‚
â”œâ”€â”€ .github/workflows/          # CI/CD automation
â”‚   â””â”€â”€ deploy.yml              # Deployment workflow
â”‚
â”œâ”€â”€ store_index.py              # Pinecone indexing script
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ template.py                 # Project template
â”œâ”€â”€ docker-compose.yaml         # Docker orchestration
â””â”€â”€ render.yaml                 # Render config
```

## ğŸ” How It Works

### 1. Document Processing (`store_index.py`)

```python
# Load PDFs
documents = load_pdf("data/")

# Split into chunks
text_chunks = text_split(documents)

# Generate embeddings
embeddings = download_hugging_face_embeddings()

# Store in Pinecone
PineconeVectorStore.from_texts(text_chunks, embeddings, index_name="medical-chatbot")
```

### 2. Query Processing (`app.py`)

```python
# User sends query via Flask
query = "What is diabetes?"

# Retrieve relevant context from Pinecone (k=2 chunks)
retriever = docsearch.as_retriever(search_kwargs={'k': 2})

# Generate response using Llama 2 with context
qa = RetrievalQA.from_chain_type(llm, retriever=retriever)
response = qa.invoke({"query": query})
```

### 3. Model Configuration

```python
llm = ChatGroq(
    model="llama-3.3-70b-versatile",  # Fast and efficient Groq model
    temperature=0.8,                   # Creativity (0=deterministic, 1=creative)
    max_tokens=1024,                   # Maximum response length
    groq_api_key=os.getenv("GROQ_API_KEY")  # API key from .env file
)
```

## ğŸ› Troubleshooting

### Issue: Token Limit Exceeded

```
Number of tokens (974) exceeded maximum context length (512)
```

**Solution**: Increase `context_length` and `max_new_tokens` in `app.py`

### Issue: Rate Limit Exceeded

```
Rate limit exceeded: 30 requests per minute
```

**Solutions**:

- Implement request queuing in Flask
- Add caching for common questions (already included)
- Upgrade to Groq paid tier if needed
- Add rate limiting on frontend

### Issue: Invalid API Key

```
AuthenticationError: Invalid API key
```

**Solution**: Verify your Groq API key in `.env` file:

```bash
GROQ_API_KEY=gsk_your_actual_key_here
```

### Issue: Conda Not Recognized

**Solution**: Initialize conda for PowerShell:

```powershell
C:\ProgramData\Anaconda3\Scripts\conda.exe init powershell
```

Then restart PowerShell.

## ğŸ¯ Performance Optimization

### Speed Improvements

Groq API is already optimized for speed (up to 10x faster than local models), but you can further optimize:

1. **Limit Tokens**: Set `max_tokens` to 512 for faster responses
2. **Fewer Retrievals**: Use `search_kwargs={'k': 1}` for single-document retrieval
3. **Use Caching**: Leverage `common_responses.py` for frequent questions
4. **Optimize Embeddings**: Cache embedding results for repeated queries

### Groq Free Tier Limits

- **Requests per minute**: 30 RPM
- **Requests per day**: 14,400 RPD
- **Tokens per minute**: ~20,000 TPM
- **Max context window**: 32,768 tokens

### Rate Limit Management

- Implement request queuing
- Add exponential backoff on rate limit errors
- Cache common responses
- Consider upgrading to paid tier for production

## ğŸ³ Docker Deployment

### Local Docker Testing

```bash
# Build the Docker image
docker build -t medbot:latest .

# Run with docker-compose
docker-compose up -d

# Check logs
docker-compose logs -f

# Stop containers
docker-compose down
```

### Deploy to Render

1. **Push code to GitHub**:

   ```bash
   git add .
   git commit -m "Add Docker and CI/CD configuration"
   git push origin main
   ```

2. **Set up Render**:

   - Go to [Render Dashboard](https://dashboard.render.com/)
   - Click "New +" â†’ "Web Service"
   - Connect your GitHub repository
   - Render will auto-detect `render.yaml`

3. **Configure Environment Variables** in Render:

   - `PINECONE_API_KEY`: Your Pinecone API key
   - `GROQ_API_KEY`: Your Groq API key

4. **CI/CD is configured** via `.github/workflows/deploy.yml`
   - Automatically deploys on push to main branch
   - Ensure GitHub secrets are configured for your deployment targets

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/AmazingFeature`)
3. Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## ğŸ™ Acknowledgments

- [Groq](https://groq.com/) - Lightning-fast LLM inference API
- [Meta AI](https://ai.meta.com/) - Llama 3.3 70B model
- [LangChain](https://www.langchain.com/) - RAG framework
- [Pinecone](https://www.pinecone.io/) - Vector database
- [HuggingFace](https://huggingface.co/) - Embeddings models

## ğŸ“§ Contact

**PuLeeNa** - [@PuLeeNa](https://github.com/PuLeeNa)

---

â­ **Star this repository if you find it helpful!**
