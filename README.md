# ğŸ¥ MedBot

## ğŸŒ Live Demo

**Try it now:** [https://medic-bot.netlify.app](https://medic-bot.netlify.app)

An intelligent medical chatbot powered by **Groq API (Llama 3.3 70B)**, **LangChain**, and **Pinecone** vector database. This RAG (Retrieval-Augmented Generation) application provides accurate medical information by retrieving relevant context from medical documents and generating fast, high-quality responses using Groq's lightning-fast inference API.

![Python](https://img.shields.io/badge/Python-3.10-blue)
![Flask](https://img.shields.io/badge/Flask-3.1.0-green)
![LangChain](https://img.shields.io/badge/LangChain-1.1.0-orange)

## âœ¨ Features

- ğŸ¤– **RAG-based Architecture**: Retrieves relevant medical information from documents before generating responses
- âš¡ **Lightning-Fast API**: Powered by Groq's ultra-fast inference (up to 10x faster than local models)
- ğŸ¯ **Vector Search**: Uses Pinecone for efficient semantic search across medical documents
- ğŸ’¬ **Interactive UI**: Clean, responsive chat interface with typing indicators
- ğŸ†“ **Free Tier Available**: Groq offers generous free API access (14,400 requests/day)
- ğŸš€ **High-Quality Responses**: Uses Llama 3.3 70B model for superior accuracy
- ğŸ“š **Document Support**: Processes PDF medical documents with PyMuPDF
- ğŸ’¡ **Smart Caching**: Common responses feature for frequently asked questions

## ğŸ—ï¸ Architecture

**Split Deployment:**

- **Frontend**: Static files on Netlify
- **Backend**: Flask API on Render (handles AI processing)

This is a **RAG (Retrieval-Augmented Generation)** chatbot:

1. **Retrieval**: Searches Pinecone vector database for relevant medical document chunks
2. **Augmentation**: Injects retrieved context into the prompt template
3. **Generation**: Groq API (Llama 3.3 70B) generates fast, accurate responses based on the context

```
User â†’ Netlify Frontend â†’ Render Backend API â†’ Pinecone â†’ Groq LLM â†’ Response
```

## ğŸ› ï¸ Tech Stack

### Core Technologies

- **LLM**: Groq API - Llama 3.3 70B Versatile
- **Framework**: LangChain 1.1.0
- **Vector DB**: Pinecone
- **Embeddings**: HuggingFace sentence-transformers (all-MiniLM-L6-v2)
- **Backend**: Flask 3.1.0
- **Model Inference**: langchain-groq 1.1.0

### Key Libraries

- `langchain-groq` - Groq API integration for fast LLM inference
- `langchain-community` - Document loaders and LLM integrations
- `langchain-pinecone` - Pinecone vector store integration
- `langchain-huggingface` - HuggingFace embeddings
- `sentence-transformers` - Text embedding models
- `pymupdf` - PDF document processing
- `python-dotenv` - Environment variable management

## ğŸ“¦ Prerequisites

- Python 3.10 or higher
- Conda (Anaconda/Miniconda) - Optional
- Pinecone API key ([Get it here](https://www.pinecone.io/))
- Groq API key ([Get free key here](https://console.groq.com/))

## ğŸš€ Quick Deploy

### Backend (Render)

1. Push to GitHub main branch
2. Auto-deploys via `deploy.yml`
3. Render will auto-detect `render.yaml` configuration

### Frontend (Netlify)

1. Push to GitHub main branch
2. Auto-deploys via Netlify configuration
3. Netlify will auto-detect `netlify.toml` configuration

## ğŸ’» Local Development

### Backend Setup

```bash
cd backend
conda create -n mchatbot python=3.10 -y
conda activate mchatbot
pip install -r requirements.txt
python app.py
```

### Frontend Setup

```bash
cd frontend
python -m http.server 8000
# Visit: http://localhost:8000
```

## âš™ï¸ Configuration

### Environment Variables

**Backend (.env)**:

```bash
PINECONE_API_KEY=your_pinecone_api_key_here
GROQ_API_KEY=your_groq_api_key_here
```

**Frontend (app.js)**:

```javascript
const API_URL = "https://your-backend-url.onrender.com";
```

## ğŸ“ Project Structure

```
MedBot/
â”‚
â”œâ”€â”€ backend/                    # Backend API
â”‚   â”œâ”€â”€ app.py                  # Flask API server
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â”œâ”€â”€ Dockerfile              # Container config
â”‚   â”œâ”€â”€ data/                   # Medical PDF documents
â”‚   â””â”€â”€ src/                    # Source modules
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ common_responses.py # Cached responses
â”‚       â”œâ”€â”€ helper.py           # Utility functions
â”‚       â””â”€â”€ prompt.py           # Prompt templates
â”‚
â”œâ”€â”€ frontend/                   # Frontend UI
â”‚   â”œâ”€â”€ index.html              # Chat interface
â”‚   â”œâ”€â”€ app.js                  # API client logic
â”‚   â”œâ”€â”€ style.css               # Styling
â”‚   â”œâ”€â”€ logo.jpg                # Logo/favicon
â”‚   â”œâ”€â”€ Dockerfile              # Container config
â”‚   â””â”€â”€ netlify.toml            # Netlify config
â”‚
â”œâ”€â”€ .github/workflows/          # CI/CD automation
â”‚   â””â”€â”€ deploy.yml              # Deployment workflow
â”‚
â”œâ”€â”€ store_index.py              # Pinecone indexing script
â”œâ”€â”€ setup.py                    # Package setup
â”œâ”€â”€ template.py                 # Project template
â”œâ”€â”€ docker-compose.yaml         # Docker orchestration
â””â”€â”€ render.yaml                 # Render config
```

## ğŸ³ Docker Deployment

### Local Docker Testing

```bash
# Build the Docker image
docker build -t medbot:latest .

# Run with docker-compose
docker-compose up -d

# Check logs
docker-compose logs -f

# Stop containers
docker-compose down
```

### Deploy to Render

1. **Push code to GitHub**:

   ```bash
   git add .
   git commit -m "Add Docker and CI/CD configuration"
   git push origin main
   ```

2. **Set up Render**:

   - Go to [Render Dashboard](https://dashboard.render.com/)
   - Click "New +" â†’ "Web Service"
   - Connect your GitHub repository
   - Render will auto-detect `render.yaml`

3. **Configure Environment Variables** in Render:

   - `PINECONE_API_KEY`: Your Pinecone API key
   - `GROQ_API_KEY`: Your Groq API key

4. **CI/CD is configured** via `.github/workflows/deploy.yml`
   - Automatically deploys on push to main branch
   - Ensure GitHub secrets are configured for your deployment targets


## ğŸ™ Acknowledgments

- [Groq](https://groq.com/) - Lightning-fast LLM inference API
- [Meta AI](https://ai.meta.com/) - Llama 3.3 70B model
- [LangChain](https://www.langchain.com/) - RAG framework
- [Pinecone](https://www.pinecone.io/) - Vector database
- [HuggingFace](https://huggingface.co/) - Embeddings models

## ğŸ“§ Contact

**PuLeeNa** - [@PuLeeNa](https://github.com/PuLeeNa)

---

â­ **Star this repository if you find it helpful!**
